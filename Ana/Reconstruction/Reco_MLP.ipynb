{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 导入依赖\n",
    "import ROOT\n",
    "import uproot\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "from scipy.stats import lognorm as log_normal_dist\n",
    "from scipy.stats import norm as gaussian_dist\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "plt.style.use('/home/llr/ilc/shi/code/Plot_style/cepc.mplstyle')\n",
    "fig_size = (8, 6)\n",
    "# 🔧 解码函数：根据 bitfield <id>calolayer:7,abslayer:1,cellid:13</id>\n",
    "def decode_volid(volid):\n",
    "    volid = int(volid)\n",
    "    calolayer = volid & 0x7F                 # bits 0–6\n",
    "    abslayer  = (volid >> 7) & 0x1           # bit 7\n",
    "    cellid    = (volid >> 8) & 0x1FFF        # bits 8–20\n",
    "    return calolayer, abslayer, cellid\n",
    "\n",
    "# 🧮 从 cellid 解出 index_x/y/z（若 cellid = z*1600 + y*40 + x）\n",
    "def decode_indices(cellid):\n",
    "    index_z = cellid // 1600\n",
    "    index_y = (cellid % 1600) // 40\n",
    "    index_x = cellid % 40\n",
    "    return index_x, index_y, index_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173155cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开 ROOT 文件，载入 events tree\n",
    "DataPath_Validate=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Validate/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60\"\n",
    "DataPath_Train=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60\"\n",
    "DataPath_Uniform=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60/Uniform\"\n",
    "Energy_Train=(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,15.0,25.0,35.0,45.0,55.0,65.0)\n",
    "#Energy_Train=(0.5,)\n",
    "Energy_Val=(0.5,1.0,2.0,5.0,10.0,20.0,30.0,40.0,50.0,60.0)\n",
    "Threshold=0.5*0.1328\n",
    "#MIP=(0.0410,0.0861,0.1328,0.1803,0.2282)# for Si 0.15mm,0.3mm,0.45mm,0.6mm,0.75mm. Unit is MeV\n",
    "ECAL_layers=30\n",
    "def ReadData_Energy(DataPath,ParticleEnergy):\n",
    "    E_layers_all = np.empty((0, ECAL_layers))\n",
    "    N_layers_all = np.empty((0, ECAL_layers))\n",
    "    E_truth_all  = np.empty((0,))\n",
    "    for i_E in range(len(ParticleEnergy)):\n",
    "    #for i_E in range(0,1):\n",
    "        input_file_name = f\"{DataPath}/{ParticleEnergy[i_E]}GeV.root\"\n",
    "        E_layers,N_layers,E_truth = ReadRoot(input_file_name)\n",
    "        E_layers_all = np.vstack((E_layers_all, np.vstack(E_layers)))\n",
    "        N_layers_all = np.vstack((N_layers_all, np.vstack(N_layers)))\n",
    "        E_truth_all  = np.concatenate((E_truth_all, E_truth))\n",
    "    return E_layers_all, N_layers_all, E_truth_all\n",
    "def ReadData_Uniform(DataPath):\n",
    "    root_files = [os.path.join(DataPath, f) for f in os.listdir(DataPath) if f.endswith(\".root\")]\n",
    "    if len(root_files) == 0:\n",
    "        raise FileNotFoundError(f\"No .root files found in {DataPath}\")\n",
    "    E_layers_all = np.empty((0, ECAL_layers))\n",
    "    N_layers_all = np.empty((0, ECAL_layers))\n",
    "    E_truth_all  = np.empty((0,))\n",
    "    for input_file in root_files:\n",
    "        E_layers, N_layers, E_truth = ReadRoot(input_file)\n",
    "        E_layers_all = np.vstack((E_layers_all, E_layers))\n",
    "        N_layers_all = np.vstack((N_layers_all, N_layers))\n",
    "        E_truth_all  = np.concatenate((E_truth_all, E_truth))\n",
    "    return E_layers_all, N_layers_all, E_truth_all\n",
    "def ReadRoot(input_file_name):\n",
    "    print(f\"Processing file: {input_file_name}\")\n",
    "    input_file = uproot.open(input_file_name)\n",
    "    tree = input_file[\"events\"]\n",
    "    cellID = tree[\"simplecaloRO.cellID\"].array(library=\"ak\")\n",
    "    energy = tree[\"simplecaloRO.energy\"].array(library=\"ak\")\n",
    "    pos_x  = tree[\"simplecaloRO.position.x\"].array(library=\"ak\")\n",
    "    pos_y  = tree[\"simplecaloRO.position.y\"].array(library=\"ak\")\n",
    "    pos_z  = tree[\"simplecaloRO.position.z\"].array(library=\"ak\")\n",
    "    #MCP_px = tree[\"MCParticles.momentum.x\"].array(library=\"np\")\n",
    "    #MCP_py = tree[\"MCParticles.momentum.y\"].array(library=\"np\")\n",
    "    #MCP_pz = tree[\"MCParticles.momentum.z\"].array(library=\"np\")\n",
    "    E_truth = tree[\"MCParticles.p0\"].array(library=\"ak\")\n",
    "    threshold_mask = energy * 1000 > Threshold\n",
    "    cellID = cellID[threshold_mask]\n",
    "    energy = energy[threshold_mask]\n",
    "    pos_x = pos_x[threshold_mask]\n",
    "    pos_y = pos_y[threshold_mask]\n",
    "    pos_z = pos_z[threshold_mask]\n",
    "    calo_layer = cellID & 0x7F\n",
    "    #print to check\n",
    "    # sorted_indices = np.argsort(calo_layer[0])\n",
    "    # energy_sorted = energy[0][sorted_indices].to_numpy()\n",
    "    # layer_sorted = calo_layer[0][sorted_indices].to_numpy()\n",
    "    # posz_sorted = pos_z[0][sorted_indices].to_numpy()\n",
    "    # print(\"Event 0 (sorted by layer):\")\n",
    "    # for l, e, z in zip(layer_sorted, energy_sorted, posz_sorted):\n",
    "    #     print(f\"  Layer {l:2d} | Energy = {e:.5f} | PosZ = {z:.5f }\")\n",
    "\n",
    "    num_events = len(energy)\n",
    "    E_layers = []\n",
    "    N_layers = []\n",
    "    for i_event in range(num_events):\n",
    "        # convert to numpy arrays\n",
    "        layers = calo_layer[i_event].to_numpy()\n",
    "        energies = energy[i_event].to_numpy()\n",
    "        E_layers_event = np.bincount(layers, weights=energies, minlength=ECAL_layers)\n",
    "        N_layers_event = np.bincount(layers, minlength=ECAL_layers)\n",
    "        E_layers.append(E_layers_event)\n",
    "        N_layers.append(N_layers_event)\n",
    "\n",
    "    #print(\"Event 0:\")\n",
    "    #for layer_idx, (E_val, N_val) in enumerate(zip(E_layers_all[0], N_layers_all[0])):\n",
    "    #    print(f\"  Layer {layer_idx:2d} | E_layer = {E_val:.5f} | N_layer = {N_val}\")\n",
    "    E_truth_flat = ak.to_numpy(ak.flatten(E_truth, axis=None))\n",
    "    return E_layers,N_layers,E_truth_flat\n",
    "#Read Data\n",
    "E_layers_train, N_layers_train, E_truth_train = ReadData_Energy(DataPath_Train,Energy_Train)\n",
    "E_layers_validate, N_layers_validate, E_truth_validate = ReadData_Energy(DataPath_Validate,Energy_Val)\n",
    "E_layers_uniform, N_layers_uniform, E_truth_uniform = ReadData_Uniform(DataPath_Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f25943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw E_truth\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.hist(E_truth_uniform, bins=1000, range=(0, 100), edgecolor=\"black\")\n",
    "plt.xlabel(\"E_truth [GeV]\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Distribution of E_truth_validate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80701cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero suppression\n",
    "def zero_suppression(E_layers, N_layers,E_truth):\n",
    "    print(len(E_layers),len(N_layers),len(E_truth))\n",
    "    N_sum = np.array([np.sum(n) for n in N_layers])\n",
    "    E_sum = np.array([np.sum(e) for e in E_layers])\n",
    "    E_truth = np.array(E_truth)\n",
    "\n",
    "    mask = N_sum > 0\n",
    "    E_layers_filtered = [E_layers[i] for i in range(len(E_layers)) if mask[i]]\n",
    "    N_layers_filtered = [N_layers[i] for i in range(len(N_layers)) if mask[i]]    \n",
    "    E_truth_filtered = E_truth[mask]\n",
    "    E_sum_filtered = E_sum[mask]\n",
    "    N_sum_filtered = N_sum[mask]\n",
    "\n",
    "    return E_layers_filtered, N_layers_filtered, E_sum_filtered, N_sum_filtered, E_truth_filtered\n",
    "\n",
    "#Start from shower layer\n",
    "def start_from_shower_layer(arr):\n",
    "    N = len(arr)\n",
    "    nonzero_idx = np.argmax(arr != 0)\n",
    "    trimmed = arr[nonzero_idx:]\n",
    "    if len(trimmed) < N:\n",
    "        trimmed = np.pad(trimmed, (0, N - len(trimmed)), 'constant')\n",
    "    return trimmed\n",
    "\n",
    "def Event_Selection(E_layers,N_layers,E_truth):\n",
    "    E_layers, N_layers, E_sum, N_sum, E_truth = zero_suppression(E_layers, N_layers, E_truth)\n",
    "    E_layers = np.array([start_from_shower_layer(ev) for ev in E_layers])\n",
    "    N_layers = np.array([start_from_shower_layer(ev) for ev in N_layers])\n",
    "    return E_layers, N_layers, E_sum, N_sum, E_truth\n",
    "\n",
    "E_layers_train, N_layers_train, E_sum_train, N_sum_train, E_truth_train = Event_Selection(E_layers_train, N_layers_train, E_truth_train)\n",
    "E_layers_uniform, N_layers_uniform, E_sum_uniform, N_sum_uniform, E_truth_uniform = Event_Selection(E_layers_uniform, N_layers_uniform, E_truth_uniform)\n",
    "E_layers_validate, N_layers_validate, E_sum_val, N_sum_val, E_truth_validate = Event_Selection(E_layers_validate, N_layers_validate, E_truth_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ==== Define MLP Model ====\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.1):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))  # output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# ==== Training function ====\n",
    "def train_mlp(X_train, y_train, X_val, y_val, \n",
    "              hidden_dims=[128, 64, 32], \n",
    "              dropout=0.1, \n",
    "              lr=1e-3, \n",
    "              batch_size=512, \n",
    "              epochs=50):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val   = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset   = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = MLPRegressor(input_dim=X_train.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        val_loss   /= len(val_dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma(x, norm, k, mu, theta):\n",
    "    return norm * gamma_dist.pdf(x, k, loc=mu, scale=theta)\n",
    "\n",
    "def fit_log_normal(x, norm, sigma, loc, scale):\n",
    "    \"\"\"Log-normal distribution with location parameter.\"\"\"\n",
    "    return norm * log_normal_dist.pdf(x, s=sigma, loc=loc, scale=scale)\n",
    "\n",
    "def fit_gaussian(x, norm, resolution, mean):\n",
    "    return norm * gaussian_dist.pdf(x, loc=mean, scale=mean * resolution)\n",
    "\n",
    "def fit_all(type,E_truth,counts,bin_edges):\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    dx = bin_edges[1] - bin_edges[0]\n",
    "    peak = bin_centers[np.argmax(counts)]\n",
    "    mean = np.average(bin_centers, weights=counts)\n",
    "    rms = np.sqrt(np.average((bin_centers - mean)**2, weights=counts))\n",
    "    print(f\"Mean: {mean}, Peak: {peak}, RMS: {rms}\")\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], alpha=0.6, label=\"Data\", color='blue')\n",
    "\n",
    "    #Gamma Fit\n",
    "    #theta = abs(mean - peak)\n",
    "    #k = rms / np.sqrt(theta)\n",
    "    #mu = E_truth\n",
    "    theta = max(rms, 1e-6)\n",
    "    k = max((mean - bin_edges[0]) / theta, 1e-3)\n",
    "    mu = bin_edges[0] # or try mean - k*theta\n",
    "    norm = counts.sum() * dx\n",
    "    params_gamma_initial = [norm, k, mu, theta]\n",
    "    lower_bounds = [norm*0.01, k*0.1, mu*0.1-1, theta*0.1]\n",
    "    upper_bounds = [norm*6, k*10, mu*1.5, theta*10]\n",
    "    print(f\"Initial Gamma Fit Parameters: {params_gamma_initial}\")\n",
    "    params_gamma, cov_gamma = curve_fit(fit_gamma, bin_centers, counts, p0=params_gamma_initial, bounds=(lower_bounds, upper_bounds))\n",
    "    print(f\"Gamma Fit Parameters: {params_gamma}\")\n",
    "    x_fit_gamma = np.linspace(bin_edges[0], bin_edges[-1], 100)\n",
    "    y_fit_gamma = fit_gamma(x_fit_gamma, *params_gamma)\n",
    "    #calculate peak and resolution\n",
    "    k,mu,theta = params_gamma[1], params_gamma[2], params_gamma[3]\n",
    "    peak_gamma = (k-1) * theta + mu\n",
    "    J = np.array([0,theta, 1.0, (k - 1.0)])\n",
    "    peak_gamma_error = np.sqrt(J @ cov_gamma @ J)\n",
    "    #res\n",
    "    res_gamma = np.sqrt(k) * theta / peak_gamma\n",
    "    dr_dnorm = 0.0\n",
    "    dr_dk = (0.5 / np.sqrt(k) * theta * peak_gamma - np.sqrt(k) * theta * (theta)) / (peak_gamma**2)\n",
    "    dr_dmu = - np.sqrt(k) * theta / (peak_gamma**2)\n",
    "    dr_dtheta = (np.sqrt(k) * peak_gamma - np.sqrt(k) * theta * (k - 1.0)) / (peak_gamma**2)\n",
    "    J = np.array([dr_dnorm, dr_dk, dr_dmu, dr_dtheta])\n",
    "    res_gamma_error = np.sqrt(J @ cov_gamma @ J)\n",
    "    print(f\"Gamma Fit Peak: {peak_gamma} ± {peak_gamma_error}, Resolution: {res_gamma} ± {res_gamma_error}\")\n",
    "    if type == \"E\":\n",
    "        plt.title(f\"Deposited Energy {E_truth} GeV\")\n",
    "        plt.xlabel(\"Deposited Energy [MeV]\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    elif type == \"N\":\n",
    "        plt.title(f\"Number of Hits {E_truth} GeV\")\n",
    "        plt.xlabel(\"Number of Hits\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    elif type == \"E_reco\":\n",
    "        plt.title(f\"Reconstructed Energy {E_truth} GeV\")\n",
    "        plt.xlabel(\"Reconstructed Energy [GeV]\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    else:\n",
    "        print(\"Unknown type\")\n",
    "        return 0\n",
    "    plt.plot(x_fit_gamma, y_fit_gamma, 'r-', label=\"Gamma Fit\")\n",
    "    #plt.xlim(peak_gamma - 4*peak_gamma*res_gamma, peak_gamma + 8*peak_gamma*res_gamma)\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (peak_gamma, peak_gamma_error, res_gamma, res_gamma_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is left here for debug\n",
    "i_E = 0  # Example index, change as needed\n",
    "print(len(Reco_hist_lightgbm))\n",
    "print(Reco_hist_lightgbm[i_E])\n",
    "peak_gamma,peak_gamma_error,res_gamma,res_gamma_error = fit_all(\"E_reco\",Reco_hist_lightgbm[i_E][\"E_truth\"],Reco_hist_lightgbm[i_E][\"counts\"],Reco_hist_lightgbm[i_E][\"bin_edges\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit RecoE and Save as CSV\n",
    "#Gauss and LogNormal were not fitted but saved as 0 to keep format\n",
    "results = []\n",
    "for i_E in range(len(Energy_Val)):\n",
    "    peak_gauss, peak_gauss_error, res_gauss, res_gauss_error = 0, 0, 0, 0\n",
    "    peak_log_normal, peak_log_normal_error, res_log_normal, res_log_normal_error = 0, 0, 0, 0\n",
    "    peak_gamma, peak_gamma_error, res_gamma, res_gamma_error = fit_all(\"E_reco\",Reco_hist_lightgbm[i_E][\"E_truth\"],Reco_hist_lightgbm[i_E][\"counts\"],Reco_hist_lightgbm[i_E][\"bin_edges\"])\n",
    "    results.append({\n",
    "        \"ParticleEnergy\": Energy_Val[i_E],\n",
    "        \"Peak_Gauss\": peak_gauss,\n",
    "        \"Peak_Gauss_Error\": peak_gauss_error,\n",
    "        \"Res_Gauss\": res_gauss,\n",
    "        \"Res_Gauss_Error\": res_gauss_error,\n",
    "        \"Peak_Gamma\": peak_gamma,\n",
    "        \"Peak_Gamma_Error\": peak_gamma_error,\n",
    "        \"Res_Gamma\": res_gamma,\n",
    "        \"Res_Gamma_Error\": res_gamma_error,\n",
    "        \"Peak_LogNormal\": peak_log_normal,\n",
    "        \"Peak_LogNormal_Error\": peak_log_normal_error,\n",
    "        \"Res_LogNormal\": res_log_normal,\n",
    "        \"Res_LogNormal_Error\": res_log_normal_error\n",
    "    })\n",
    "df = pd.DataFrame(results)\n",
    "outputfile_name = os.path.join(DataPath_Validate, \"RecoE_lightgbm_Fit.csv\")\n",
    "df.to_csv(outputfile_name, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
