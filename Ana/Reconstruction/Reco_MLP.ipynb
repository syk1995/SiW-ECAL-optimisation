{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b68282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ ÂØºÂÖ•‰æùËµñ\n",
    "import ROOT\n",
    "import uproot\n",
    "import math\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "from scipy.stats import lognorm as log_normal_dist\n",
    "from scipy.stats import norm as gaussian_dist\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('/home/llr/ilc/shi/code/Plot_style/cepc.mplstyle')\n",
    "fig_size = (8, 6)\n",
    "# üîß Ëß£Á†ÅÂáΩÊï∞ÔºöÊ†πÊçÆ bitfield <id>calolayer:7,abslayer:1,cellid:13</id>\n",
    "def decode_volid(volid):\n",
    "    volid = int(volid)\n",
    "    calolayer = volid & 0x7F                 # bits 0‚Äì6\n",
    "    abslayer  = (volid >> 7) & 0x1           # bit 7\n",
    "    cellid    = (volid >> 8) & 0x1FFF        # bits 8‚Äì20\n",
    "    return calolayer, abslayer, cellid\n",
    "\n",
    "# üßÆ ‰ªé cellid Ëß£Âá∫ index_x/y/zÔºàËã• cellid = z*1600 + y*40 + xÔºâ\n",
    "def decode_indices(cellid):\n",
    "    index_z = cellid // 1600\n",
    "    index_y = (cellid % 1600) // 40\n",
    "    index_x = cellid % 40\n",
    "    return index_x, index_y, index_z\n",
    "def exp_func(x, a, b, c):\n",
    "    return a * np.exp(b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173155cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60/0.5GeV.root\n",
      "Processing file: /home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60/1.5GeV.root\n",
      "Processing file: /home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60/2.5GeV.root\n"
     ]
    }
   ],
   "source": [
    "# ÊâìÂºÄ ROOT Êñá‰ª∂ÔºåËΩΩÂÖ• events tree\n",
    "DataPath_Validate=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Validate/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60\"\n",
    "DataPath_Train=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60\"\n",
    "DataPath_Uniform=\"/home/llr/ilc/shi/data/SiWECAL-Prototype/Simu2025-06/CONF0/gamma/Train/Merged_X5.0mm_Y5.0mm_Si0.45mm_layer30_in60/Uniform\"\n",
    "Energy_Train=(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,15.0,25.0,35.0,45.0,55.0,65.0)\n",
    "#Energy_Train=(0.5,)\n",
    "Energy_test=(0.5,1.0,2.0,5.0,10.0,20.0,30.0,40.0,50.0,60.0)\n",
    "Threshold=0.5*0.1328\n",
    "#MIP=(0.0410,0.0861,0.1328,0.1803,0.2282)# for Si 0.15mm,0.3mm,0.45mm,0.6mm,0.75mm. Unit is MeV\n",
    "ECAL_layers=30\n",
    "def ReadData_Energy(DataPath,ParticleEnergy):\n",
    "    E_layers_all = np.empty((0, ECAL_layers))\n",
    "    N_layers_all = np.empty((0, ECAL_layers))\n",
    "    E_truth_all  = np.empty((0,))\n",
    "    for i_E in range(len(ParticleEnergy)):\n",
    "    #for i_E in range(0,1):\n",
    "        input_file_name = f\"{DataPath}/{ParticleEnergy[i_E]}GeV.root\"\n",
    "        E_layers,N_layers,E_truth = ReadRoot(input_file_name)\n",
    "        E_layers_all = np.vstack((E_layers_all, np.vstack(E_layers)))\n",
    "        N_layers_all = np.vstack((N_layers_all, np.vstack(N_layers)))\n",
    "        E_truth_all  = np.concatenate((E_truth_all, E_truth))\n",
    "    return E_layers_all, N_layers_all, E_truth_all\n",
    "def ReadData_Uniform(DataPath):\n",
    "    root_files = [os.path.join(DataPath, f) for f in os.listdir(DataPath) if f.endswith(\".root\")]\n",
    "    if len(root_files) == 0:\n",
    "        raise FileNotFoundError(f\"No .root files found in {DataPath}\")\n",
    "    E_layers_all = np.empty((0, ECAL_layers))\n",
    "    N_layers_all = np.empty((0, ECAL_layers))\n",
    "    E_truth_all  = np.empty((0,))\n",
    "    for input_file in root_files:\n",
    "        E_layers, N_layers, E_truth = ReadRoot(input_file)\n",
    "        E_layers_all = np.vstack((E_layers_all, E_layers))\n",
    "        N_layers_all = np.vstack((N_layers_all, N_layers))\n",
    "        E_truth_all  = np.concatenate((E_truth_all, E_truth))\n",
    "    return E_layers_all, N_layers_all, E_truth_all\n",
    "def ReadRoot(input_file_name):\n",
    "    print(f\"Processing file: {input_file_name}\")\n",
    "    input_file = uproot.open(input_file_name)\n",
    "    tree = input_file[\"events\"]\n",
    "    cellID = tree[\"simplecaloRO.cellID\"].array(library=\"ak\")\n",
    "    energy = tree[\"simplecaloRO.energy\"].array(library=\"ak\")\n",
    "    pos_x  = tree[\"simplecaloRO.position.x\"].array(library=\"ak\")\n",
    "    pos_y  = tree[\"simplecaloRO.position.y\"].array(library=\"ak\")\n",
    "    pos_z  = tree[\"simplecaloRO.position.z\"].array(library=\"ak\")\n",
    "    #MCP_px = tree[\"MCParticles.momentum.x\"].array(library=\"np\")\n",
    "    #MCP_py = tree[\"MCParticles.momentum.y\"].array(library=\"np\")\n",
    "    #MCP_pz = tree[\"MCParticles.momentum.z\"].array(library=\"np\")\n",
    "    E_truth = tree[\"MCParticles.p0\"].array(library=\"ak\")\n",
    "    threshold_mask = energy * 1000 > Threshold\n",
    "    cellID = cellID[threshold_mask]\n",
    "    energy = energy[threshold_mask]\n",
    "    pos_x = pos_x[threshold_mask]\n",
    "    pos_y = pos_y[threshold_mask]\n",
    "    pos_z = pos_z[threshold_mask]\n",
    "    calo_layer = cellID & 0x7F\n",
    "    #print to check\n",
    "    # sorted_indices = np.argsort(calo_layer[0])\n",
    "    # energy_sorted = energy[0][sorted_indices].to_numpy()\n",
    "    # layer_sorted = calo_layer[0][sorted_indices].to_numpy()\n",
    "    # posz_sorted = pos_z[0][sorted_indices].to_numpy()\n",
    "    # print(\"Event 0 (sorted by layer):\")\n",
    "    # for l, e, z in zip(layer_sorted, energy_sorted, posz_sorted):\n",
    "    #     print(f\"  Layer {l:2d} | Energy = {e:.5f} | PosZ = {z:.5f }\")\n",
    "\n",
    "    num_events = len(energy)\n",
    "    E_layers = []\n",
    "    N_layers = []\n",
    "    for i_event in range(num_events):\n",
    "        # convert to numpy arrays\n",
    "        layers = calo_layer[i_event].to_numpy()\n",
    "        energies = energy[i_event].to_numpy()\n",
    "        E_layers_event = np.bincount(layers, weights=energies, minlength=ECAL_layers)\n",
    "        N_layers_event = np.bincount(layers, minlength=ECAL_layers)\n",
    "        E_layers.append(E_layers_event)\n",
    "        N_layers.append(N_layers_event)\n",
    "\n",
    "    #print(\"Event 0:\")\n",
    "    #for layer_idx, (E_val, N_val) in enumerate(zip(E_layers_all[0], N_layers_all[0])):\n",
    "    #    print(f\"  Layer {layer_idx:2d} | E_layer = {E_val:.5f} | N_layer = {N_val}\")\n",
    "    E_truth_flat = ak.to_numpy(ak.flatten(E_truth, axis=None))\n",
    "    return E_layers,N_layers,E_truth_flat\n",
    "#Read Data\n",
    "E_layers_train, N_layers_train, E_truth_train = ReadData_Energy(DataPath_Train,Energy_Train)\n",
    "E_layers_test, N_layers_test, E_truth_test = ReadData_Energy(DataPath_Validate,Energy_test)\n",
    "E_layers_uniform, N_layers_uniform, E_truth_uniform = ReadData_Uniform(DataPath_Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f25943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw E_truth\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.hist(E_truth_uniform, bins=1000, range=(0, 100), edgecolor=\"black\")\n",
    "plt.xlabel(\"E_truth [GeV]\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Distribution of E_truth_test\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80701cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero suppression\n",
    "def zero_suppression(E_layers, N_layers,E_truth):\n",
    "    N_sum = np.array([np.sum(n) for n in N_layers])\n",
    "    E_sum = np.array([np.sum(e) for e in E_layers])\n",
    "    E_truth = np.array(E_truth)\n",
    "\n",
    "    mask = N_sum > 0\n",
    "    E_layers_filtered = [E_layers[i] for i in range(len(E_layers)) if mask[i]]\n",
    "    N_layers_filtered = [N_layers[i] for i in range(len(N_layers)) if mask[i]]    \n",
    "    E_truth_filtered = E_truth[mask]\n",
    "    E_sum_filtered = E_sum[mask]\n",
    "    N_sum_filtered = N_sum[mask]\n",
    "\n",
    "    print(\"length after zero suppression:\", len(E_layers_filtered), len(N_layers_filtered), len(E_sum_filtered), len(N_sum_filtered), len(E_truth_filtered))\n",
    "    return E_layers_filtered, N_layers_filtered, E_sum_filtered, N_sum_filtered, E_truth_filtered\n",
    "\n",
    "#Start from shower layer\n",
    "def start_from_shower_layer(arr):\n",
    "    N = len(arr)\n",
    "    nonzero_idx = np.argmax(arr != 0)\n",
    "    trimmed = arr[nonzero_idx:]\n",
    "    if len(trimmed) < N:\n",
    "        trimmed = np.pad(trimmed, (0, N - len(trimmed)), 'constant')\n",
    "    return trimmed\n",
    "\n",
    "def Event_Selection(E_layers,N_layers,E_truth):\n",
    "    E_layers, N_layers, E_sum, N_sum, E_truth = zero_suppression(E_layers, N_layers, E_truth)\n",
    "    E_layers = np.array([start_from_shower_layer(ev) for ev in E_layers])\n",
    "    N_layers = np.array([start_from_shower_layer(ev) for ev in N_layers])\n",
    "    return E_layers, N_layers, E_sum, N_sum, E_truth\n",
    "\n",
    "E_layers_train, N_layers_train, E_sum_train, N_sum_train, E_truth_train = Event_Selection(E_layers_train, N_layers_train, E_truth_train)\n",
    "E_layers_uniform, N_layers_uniform, E_sum_uniform, N_sum_uniform, E_truth_uniform = Event_Selection(E_layers_uniform, N_layers_uniform, E_truth_uniform)\n",
    "E_layers_test, N_layers_test, E_sum_test, N_sum_test, E_truth_test = Event_Selection(E_layers_test, N_layers_test, E_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binned(x, y, num_bins=30, color=\"blue\", label=None):\n",
    "    \"\"\"Plot mean y in bins of x.\"\"\"\n",
    "    bin_means, bin_edges, _ = binned_statistic(x, y, statistic='mean', bins=num_bins)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    plt.plot(bin_centers, bin_means, marker=\"o\", linestyle=\"None\", color=color, label=label)\n",
    "\n",
    "def exp_func(x, a, b, c):\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "fit_range = (0, 65)\n",
    "mask_fit = (E_truth_uniform > fit_range[0]) & (E_truth_uniform < fit_range[1])\n",
    "# E_sum: linear\n",
    "coeffs_calibE = np.polyfit(E_sum_uniform[mask_fit], E_truth_uniform[mask_fit], 1)\n",
    "coeffs_calibE_inv = np.polyfit(E_truth_uniform[mask_fit], E_sum_uniform[mask_fit], 1)\n",
    "E_fit_line = np.poly1d(coeffs_calibE_inv)\n",
    "# N_sum: exponential\n",
    "coeffs_calibN, _ = curve_fit(exp_func, N_sum_uniform[mask_fit], E_truth_uniform[mask_fit], p0=(1,0.01,1))\n",
    "E_fit = exp_func(N_sum_uniform, *coeffs_calibN)\n",
    "interp_inv = interp1d(E_fit, N_sum_uniform, bounds_error=False, fill_value=\"extrapolate\")\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_binned(E_truth_uniform, E_sum_uniform, color=\"blue\", label=\"Binned mean\")\n",
    "X_line=np.linspace(fit_range[0], fit_range[1], 200)\n",
    "plt.plot(X_line, E_fit_line(X_line), color=\"red\", linestyle=\"--\", label=\"Fit Line\")\n",
    "plt.ylabel(\"E_sum_uniform\")\n",
    "plt.xlabel(\"E_truth_uniform\")\n",
    "plt.title(\"Uniform samples\")\n",
    "plt.grid(True, linestyle=\"-\", alpha=0.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_binned(E_truth_uniform, N_sum_uniform, color=\"orange\", label=\"Binned mean\")\n",
    "E_vals = np.linspace(fit_range[0], fit_range[1], 200)\n",
    "N_vals = interp_inv(E_vals)  # Áî®ÂèçÂáΩÊï∞ÂæóÂà∞ N_sum ÂØπÂ∫îÁöÑÊãüÂêàÂÄº\n",
    "plt.plot(E_vals, N_vals, color=\"red\", linestyle=\"--\", label=\"Fit Line\")\n",
    "plt.ylabel(\"N_sum_uniform\")\n",
    "plt.xlabel(\"E_truth_uniform\")\n",
    "plt.title(\"Uniform samples\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "E_calib = np.poly1d(coeffs_calibE)\n",
    "E_sum_uniform_calib = E_calib(E_sum_uniform)\n",
    "E_sum_train_calib   = E_calib(E_sum_train)\n",
    "E_sum_test_calib    = E_calib(E_sum_test)\n",
    "N_sum_uniform_calib = exp_func(N_sum_uniform, *coeffs_calibN)\n",
    "N_sum_train_calib   = exp_func(N_sum_train, *coeffs_calibN)\n",
    "N_sum_test_calib    = exp_func(N_sum_test, *coeffs_calibN)\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_binned(E_sum_uniform_calib,E_truth_uniform, color=\"blue\", label=\"Binned mean\")\n",
    "plt.xlabel(\"E_sum_uniform_calib\")\n",
    "plt.ylabel(\"E_truth_uniform\")\n",
    "plt.title(\"Calibration (E_sum)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_binned(N_sum_uniform_calib, E_truth_uniform, color=\"orange\", label=\"Binned mean\")\n",
    "plt.xlabel(\"N_sum_uniform_calib\")\n",
    "plt.ylabel(\"E_truth_uniform\")\n",
    "plt.title(\"Calibration (N_sum)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combined_fit(x, y, fit_range=(0,65), plot=True, num_bins=100):\n",
    "    # Step 1: Linear\n",
    "    linear_range = (10,60)\n",
    "    mask_lin = (x > linear_range[0]) & (x < linear_range[1])\n",
    "    coeffs_lin = np.polyfit(x[mask_lin], y[mask_lin], 1)\n",
    "    y_lin = np.poly1d(coeffs_lin)(x)\n",
    "    residual = y - y_lin\n",
    "    print(\"Step 1 Linear fit coeffs:\", coeffs_lin)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plot_binned(x, y, num_bins=num_bins, color=\"blue\", label=\"Data (bin mean)\")\n",
    "        plt.plot(x[mask_lin], y_lin[mask_lin], color=\"red\", linestyle=\"--\", label=\"Step1: Linear\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(\"Step 1: Linear Fit\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    # Step 2: exp fit\n",
    "    exp_range1 = (0,10)\n",
    "    mask_fit1 = (x > exp_range1[0]) & (x < exp_range1[1])\n",
    "    exp_eff1,_ = curve_fit(exp_func, x[mask_fit1], residual[mask_fit1], p0=(1,-1,1))\n",
    "    residual_exp_fit1 = exp_func(x, *exp_eff1)\n",
    "    print(\"Step 2 exp fit coeffs:\", exp_eff1)\n",
    "    exp_range2 = (60, 65)\n",
    "    mask_fit2 = (x > exp_range2[0]) & (x < exp_range2[1])\n",
    "    def exp_func2(x, a, b, c,d):\n",
    "        return a * np.exp(b * (c-x)) + d\n",
    "    exp_eff2,_ = curve_fit(exp_func2, x[mask_fit2], residual[mask_fit2], p0=(0.5,-0.5,70,0))\n",
    "    residual_exp_fit2 = exp_func2(x, *exp_eff2) - exp_eff2[3]\n",
    "    print(\"Step 2 exp fit coeffs (high end):\", exp_eff2)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plot_binned(x, residual, num_bins=num_bins, color=\"blue\", label=\"Residual (bin mean)\")\n",
    "        plt.plot(x[mask_fit1], residual_exp_fit1[mask_fit1], color=\"red\", linestyle=\"-\", label=\"Step2: exp Fit\")\n",
    "        plt.plot(x[mask_fit2], residual_exp_fit2[mask_fit2], color=\"orange\", linestyle=\"-\", label=\"Step2: exp Fit (high end)\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"Residual\")\n",
    "        plt.title(\"Step 2: exp Fit Residual\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    residual = y - residual_exp_fit1 - residual_exp_fit2\n",
    "\n",
    "    # Step 3: Linear again\n",
    "    mask_fit = (x > fit_range[0]) & (x < fit_range[1])\n",
    "    params_lin2 = np.polyfit(x[mask_fit], residual[mask_fit], 1)\n",
    "    y_lin2 = np.poly1d(params_lin2)(x)\n",
    "    print(\"Step 3 Linear fit coeffs:\", params_lin2)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plot_binned(x, residual, num_bins=num_bins, color=\"blue\", label=\"Residual after Step2 (bin mean)\")\n",
    "        plt.plot(x, y_lin2, color=\"red\", linestyle=\"--\", label=\"Step3: Linear Fit\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"Residual\")\n",
    "        plt.title(\"Step 3: Linear Fit Residual\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    # Final combined function\n",
    "    def f(x_val):\n",
    "        return exp_func(x_val, *exp_eff1) + exp_func2(x_val, *exp_eff2) + (params_lin2[0] * x_val + params_lin2[1])\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "fit_range = (0, 70)\n",
    "residual_E = (E_sum_uniform_calib - E_truth_uniform)/E_sum_uniform_calib\n",
    "residual_E_fit = Combined_fit(E_sum_uniform_calib, residual_E, fit_range)\n",
    "plt.figure(figsize=(12,5))\n",
    "plot_binned(E_sum_uniform_calib, residual_E, color=\"blue\", label=\"Residual\")\n",
    "X_line=np.linspace(fit_range[0], fit_range[1], 200)\n",
    "plt.plot(X_line, residual_E_fit(X_line), color=\"red\", linestyle=\"--\", label=\"Fit Curve\")\n",
    "plt.xlabel(\"E_sum_uniform\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual vs E_sum (E_sum)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "residual_value = residual_E_fit(E_sum_uniform_calib)\n",
    "residual_value = np.clip(residual_value, None, 0.15)\n",
    "E_sum_uniform_calib2 = E_sum_uniform_calib * (1 - residual_value)\n",
    "residual_value = residual_E_fit(E_sum_train_calib)\n",
    "residual_value = np.clip(residual_value, None, 0.15)\n",
    "E_sum_train_calib2 = E_sum_train_calib * (1 - residual_value)\n",
    "residual_value = residual_E_fit(E_sum_test_calib)\n",
    "residual_value = np.clip(residual_value, None, 0.15)\n",
    "E_sum_test_calib2 = E_sum_test_calib * (1 - residual_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be032de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Prepare inputs for MLP ====\n",
    "#ratio\n",
    "E_ratio_train   = [layer / layer.sum() for layer in E_layers_train]\n",
    "E_ratio_val     = [layer / layer.sum() for layer in E_layers_test]\n",
    "E_ratio_uniform = [layer / layer.sum() for layer in E_layers_uniform]\n",
    "N_ratio_train   = [layer / layer.sum() for layer in N_layers_train]\n",
    "N_ratio_val     = [layer / layer.sum() for layer in N_layers_test]\n",
    "N_ratio_uniform = [layer / layer.sum() for layer in N_layers_uniform]\n",
    "#log\n",
    "E_log_train = np.log(E_layers_train + 1)\n",
    "E_log_test = np.log(E_layers_test + 1)\n",
    "E_log_uniform = np.log(E_layers_uniform + 1)\n",
    "N_log_train = np.log(N_layers_train + 1)\n",
    "N_log_test = np.log(N_layers_test + 1)\n",
    "N_log_uniform = np.log(N_layers_uniform + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedRelativeMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8, power=1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.power = power  # ÊéßÂà∂ÊùÉÈáç‰∏ãÈôçÈÄüÂ∫¶\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        weight = 1.0 / (y_true + self.eps) ** self.power\n",
    "        return torch.mean(weight * ((y_true - y_pred) / (y_true + self.eps)) ** 2)\n",
    "class RelativeMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        rel = (y_pred - y_true) / (y_true + self.eps)\n",
    "        return torch.mean(rel**2)\n",
    "class RelativeMSELoss_Power3(nn.Module):\n",
    "    def __init__(self, eps=1e-8, power=3):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.power = power\n",
    "    def forward(self, y_pred, y_true):\n",
    "        out = ((y_pred - y_true)**2) / ((y_true + self.eps)**self.power)\n",
    "        return torch.mean(out)\n",
    "    \n",
    "class HuberRelativeLoss(nn.Module):\n",
    "    def __init__(self, delta=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        rel = (y_pred - y_true) / (y_true + self.eps)\n",
    "        abs_rel = torch.abs(rel)\n",
    "        loss = torch.where(abs_rel < self.delta, 0.5 * rel**2, self.delta * (abs_rel - 0.5*self.delta))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1, activation=None):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = activation if activation is not None else nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + identity\n",
    "        return out\n",
    "\n",
    "# ===== MLP ÂõûÂΩíÊ®°Âûã =====\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.1, activation=nn.PReLU()):\n",
    "        super().__init__()\n",
    "        print(f\"Hidden dims: {hidden_dims}, Dropout: {dropout}, Activation: {activation}\")\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(ResidualLinearBlock(prev_dim, h, dropout=dropout, activation=activation))\n",
    "            prev_dim = h\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.output_layer(x).squeeze(-1)\n",
    "\n",
    "# ==== Training function ====\n",
    "def train_mlp(X_train,y_train,X_validate,y_validate,\n",
    "            activation=nn.Sigmoid(),\n",
    "            loss=HuberRelativeLoss(),\n",
    "              hidden_dims=[128, 64, 32], \n",
    "              dropout=0.1, \n",
    "              lr=1e-3, \n",
    "              batch_size=4096, \n",
    "              epochs=500,\n",
    "              patience=10,\n",
    "              val_split=0.2,\n",
    "              random_state=42):\n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Random state: {random_state}\")\n",
    "    print(f\"Training samples: {len(X_train)}, Validation samples: {int(len(X_validate))}\")\n",
    "    print(f\"learning rate: {lr}, batch size: {batch_size}, epochs: {epochs}, patience: {patience}\")\n",
    "    print(f\"Loss function: {loss}\")\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_validate   = torch.tensor(X_validate, dtype=torch.float32)\n",
    "    y_validate   = torch.tensor(y_validate, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset   = TensorDataset(X_validate, y_validate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = MLPRegressor(input_dim=X_train.shape[1], hidden_dims=hidden_dims, dropout=dropout,\n",
    "                         activation=activation).to(device)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = HuberRelativeLoss(delta=0.05)\n",
    "\n",
    "    warmup_epochs = 5\n",
    "    base_lr = lr\n",
    "    min_lr = 1e-5\n",
    "\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return float(current_epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            progress = 5 * (current_epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))  # ËåÉÂõ¥ 0~1\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    # === Training loop with best model tracking ===\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state_dict = None\n",
    "    train_loss_history, val_loss_history, beta_history = [],  [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        scheduler.step()\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        val_loss   /= len(val_dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}| LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Nan stopping\n",
    "        if np.isnan(train_loss) or np.isnan(val_loss):\n",
    "            print(\"NaN encountered. Stopping training.\")\n",
    "            break\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state_dict = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    # Restore best model\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (Relative MSE)\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return model\n",
    "    \n",
    "def build_reco_hist(y_pred, y_truth, Energy_test, Nbins=50):\n",
    "    reco_hist = []\n",
    "    for i_E, E_truth in enumerate(Energy_test):\n",
    "        mask = (y_truth == E_truth)\n",
    "        reco_energy = y_pred[mask]\n",
    "\n",
    "        if len(reco_energy) == 0:\n",
    "            continue\n",
    "\n",
    "        mean = np.mean(reco_energy)\n",
    "        rms = np.std(reco_energy)\n",
    "        valid_mask = (reco_energy >= mean - 5*rms) & (reco_energy <= mean + 5*rms)\n",
    "\n",
    "        filtered_data = reco_energy[valid_mask]\n",
    "        counts, bin_edges = np.histogram(filtered_data, bins=Nbins)\n",
    "\n",
    "        reco_hist.append({\n",
    "            \"E_truth\": E_truth,\n",
    "            \"counts\": counts,\n",
    "            \"bin_edges\": bin_edges\n",
    "        })\n",
    "    return reco_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.hstack([E_layers_uniform, N_layers_uniform])\n",
    "#X_val = np.hstack([E_layers_test, N_layers_test])\n",
    "#X_train = np.hstack([E_layers_uniform, E_ratio_uniform, E_sum_uniform[:, np.newaxis], N_layers_uniform, N_ratio_uniform, N_sum_uniform[:, np.newaxis]])\n",
    "#X_val = np.hstack([E_layers_test, E_ratio_val, E_sum_test[:, np.newaxis], N_layers_test, N_ratio_val, N_sum_test[:, np.newaxis]])\n",
    "#X_train = np.hstack([E_layers_uniform,E_log_uniform, E_sum_uniform[:, np.newaxis],N_layers_uniform,N_log_uniform,N_sum_uniform[:,np.newaxis]])\n",
    "#X_val = np.hstack([E_layers_test,E_log_test,E_sum_test[:, np.newaxis], N_layers_test,N_log_test,N_sum_test[:,np.newaxis]])\n",
    "\n",
    "X_train = np.hstack([E_layers_uniform, E_sum_uniform_calib[:,np.newaxis],N_layers_uniform, N_sum_uniform_calib[:,np.newaxis]])\n",
    "X_test = np.hstack([E_layers_test, E_sum_test_calib[:,np.newaxis], N_layers_test, N_sum_test_calib[:,np.newaxis]])\n",
    "y_train = E_truth_uniform\n",
    "y_test = E_truth_test\n",
    "\n",
    "#Scaling\n",
    "random_state = 42\n",
    "val_split = 0.2\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=val_split, random_state=random_state)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validate = scaler.transform(X_validate)\n",
    "\n",
    "def plot_features_in_batches(X, feature_names=None, bins=50, n_cols=2, figsize=(12, 6)):\n",
    "    n_features = X.shape[1]\n",
    "    n_rows = (n_cols + n_cols - 1) // n_cols  # ÊØèÈ°µÂè™Êúâ 1 Ë°å 2 ÂàóÁöÑËØù n_rows=1\n",
    "    for start in range(0, n_features, n_cols):\n",
    "        end = min(start + n_cols, n_features)\n",
    "        plt.figure(figsize=figsize)\n",
    "        for i, j in enumerate(range(start, end)):\n",
    "            plt.subplot(1, n_cols, i + 1)\n",
    "            plt.hist(X[:, j], bins=bins, color='skyblue', edgecolor='black')\n",
    "            plt.yscale('log')\n",
    "            if feature_names:\n",
    "                plt.title(feature_names[j], fontsize=10)\n",
    "            else:\n",
    "                plt.title(f\"Feature {j}\", fontsize=10)\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "        plt.show()\n",
    "#plot_features_in_batches(X_train, bins=50, n_cols=2, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ParameterScan=[1e-3,]#not used, but the loop is kept for future parameter scanning\n",
    "Reco_hist_mlp_all = []\n",
    "for lr in ParameterScan:\n",
    "  mlp_model= train_mlp(X_train, y_train,X_validate, y_validate,\n",
    "                    activation=nn.PReLU(),\n",
    "                    hidden_dims=[128, 64, 32],\n",
    "                      dropout=0.1,\n",
    "                      loss=HuberRelativeLoss(delta=0.05),\n",
    "                      lr=lr,\n",
    "                      batch_size=4096,\n",
    "                      epochs=50,\n",
    "                      patience=5,\n",
    "                      random_state=random_state)\n",
    "  device = next(mlp_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_validate_tensor = torch.tensor(X_validate, dtype=torch.float32).to(device)\n",
    "y_pred = mlp_model(X_validate_tensor).detach().cpu().numpy()\n",
    "y_true = y_validate\n",
    "\n",
    "fit_range = (0, 70)\n",
    "residual_E = (y_pred - y_true)/y_pred\n",
    "residual_E_fit = Combined_fit(y_pred, residual_E, fit_range)\n",
    "plt.figure(figsize=(12,5))\n",
    "plot_binned(y_pred, residual_E, color=\"blue\", label=\"Residual\")\n",
    "X_line=np.linspace(fit_range[0], fit_range[1], 200)\n",
    "plt.plot(X_line, residual_E_fit(X_line), color=\"red\", linestyle=\"--\", label=\"Fit Curve\")\n",
    "plt.xlabel(\"E pred\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual vs E pred\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "residual_value = residual_E_fit(y_pred)\n",
    "residual_value = np.clip(residual_value, None, 0.15)\n",
    "y_pred_calib = y_pred * (1 - residual_value)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test_mlp = mlp_model(X_test_tensor).cpu().numpy()\n",
    "    residual_value = residual_E_fit(y_pred_test_mlp)\n",
    "    residual_value = np.clip(residual_value, None, 0.15)\n",
    "    y_pred_test_mlp_calib = y_pred_test_mlp * (1 - residual_value)\n",
    "\n",
    "Reco_hist_mlp = build_reco_hist(y_pred_test_mlp_calib, E_truth_test, Energy_test, Nbins=50)\n",
    "Reco_hist_mlp_all.append(Reco_hist_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma(x, norm, k, mu, theta):\n",
    "    return norm * gamma_dist.pdf(x, k, loc=mu, scale=theta)\n",
    "\n",
    "def fit_log_normal(x, norm, sigma, loc, scale):\n",
    "    \"\"\"Log-normal distribution with location parameter.\"\"\"\n",
    "    return norm * log_normal_dist.pdf(x, s=sigma, loc=loc, scale=scale)\n",
    "\n",
    "def fit_gaussian(x, norm, resolution, mean):\n",
    "    return norm * gaussian_dist.pdf(x, loc=mean, scale=mean * resolution)\n",
    "\n",
    "def fit_all(type,E_truth,counts,bin_edges):\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    dx = bin_edges[1] - bin_edges[0]\n",
    "    peak = bin_centers[np.argmax(counts)]\n",
    "    mean = np.average(bin_centers, weights=counts)\n",
    "    rms = np.sqrt(np.average((bin_centers - mean)**2, weights=counts))\n",
    "    print(f\"Mean: {mean}, Peak: {peak}, RMS: {rms}\")\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], alpha=0.6, label=\"Data\", color='blue')\n",
    "\n",
    "    #Gamma Fit\n",
    "    #theta = abs(mean - peak)\n",
    "    #k = rms / np.sqrt(theta)\n",
    "    #mu = E_truth\n",
    "    theta = max(rms, 1e-6)\n",
    "    k = max((mean - bin_edges[0]) / theta, 1e-3)\n",
    "    mu = bin_edges[0] # or try mean - k*theta\n",
    "    norm = counts.sum() * dx\n",
    "    params_gamma_initial = [norm, k, mu, theta]\n",
    "    lower_bounds = [norm*0.01, k*0.1-1, mu*0.1-1, theta*0.1]\n",
    "    upper_bounds = [norm*6, k*10, mu*1.5+10, theta*10]\n",
    "    print(f\"Initial Gamma Fit Parameters: {params_gamma_initial}\")\n",
    "    print(f\"Gamma Fit Bounds: lower {lower_bounds}, \\n upper {upper_bounds}\")\n",
    "    params_gamma, cov_gamma = curve_fit(fit_gamma, bin_centers, counts, p0=params_gamma_initial, bounds=(lower_bounds, upper_bounds))\n",
    "    print(f\"Gamma Fit Parameters: {params_gamma}\")\n",
    "    x_fit_gamma = np.linspace(bin_edges[0], bin_edges[-1], 100)\n",
    "    y_fit_gamma = fit_gamma(x_fit_gamma, *params_gamma)\n",
    "    #calculate peak and resolution\n",
    "    k,mu,theta = params_gamma[1], params_gamma[2], params_gamma[3]\n",
    "    peak_gamma = (k-1) * theta + mu\n",
    "    J = np.array([0,theta, 1.0, (k - 1.0)])\n",
    "    peak_gamma_error = np.sqrt(J @ cov_gamma @ J)\n",
    "    mean_gamma = k * theta + mu\n",
    "    J = np.array([0,theta, 1.0, k])\n",
    "    mean_gamma_error = np.sqrt(J @ cov_gamma @ J)\n",
    "    #res\n",
    "    res_gamma = np.sqrt(k) * theta / peak_gamma\n",
    "    dr_dnorm = 0.0\n",
    "    dr_dk = (0.5 / np.sqrt(k) * theta * peak_gamma - np.sqrt(k) * theta * (theta)) / (peak_gamma**2)\n",
    "    dr_dmu = - np.sqrt(k) * theta / (peak_gamma**2)\n",
    "    dr_dtheta = (np.sqrt(k) * peak_gamma - np.sqrt(k) * theta * (k - 1.0)) / (peak_gamma**2)\n",
    "    J = np.array([dr_dnorm, dr_dk, dr_dmu, dr_dtheta])\n",
    "    res_gamma_error = np.sqrt(J @ cov_gamma @ J)\n",
    "    print(f\"Gamma Fit Peak: {peak_gamma} ¬± {peak_gamma_error}, Resolution: {res_gamma} ¬± {res_gamma_error}\")\n",
    "    if type == \"E\":\n",
    "        plt.title(f\"Deposited Energy {E_truth} GeV\")\n",
    "        plt.xlabel(\"Deposited Energy [MeV]\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    elif type == \"N\":\n",
    "        plt.title(f\"Number of Hits {E_truth} GeV\")\n",
    "        plt.xlabel(\"Number of Hits\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    elif type == \"E_reco\":\n",
    "        plt.title(f\"Reconstructed Energy {E_truth} GeV\")\n",
    "        plt.xlabel(\"Reconstructed Energy [GeV]\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "    else:\n",
    "        print(\"Unknown type\")\n",
    "        return 0\n",
    "    plt.plot(x_fit_gamma, y_fit_gamma, 'r-', label=\"Gamma Fit\")\n",
    "    #plt.xlim(peak_gamma - 4*peak_gamma*res_gamma, peak_gamma + 8*peak_gamma*res_gamma)\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (mean_gamma, mean_gamma_error, peak_gamma, peak_gamma_error, res_gamma, res_gamma_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is left here for debug\n",
    "Reco_hist_mlp = Reco_hist_mlp_all[0] # Example for first beta\n",
    "i_E = 0 # Example index, change as needed\n",
    "#print(len(Reco_hist_mlp))\n",
    "#print(Reco_hist_mlp[i_E])\n",
    "mean_gamma,mean_gamma_error,peak_gamma,peak_gamma_error,res_gamma,res_gamma_error = fit_all(\n",
    "    \"E_reco\",Reco_hist_mlp[i_E][\"E_truth\"],Reco_hist_mlp[i_E][\"counts\"],Reco_hist_mlp[i_E][\"bin_edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit RecoE and Save as CSV\n",
    "#Gauss and LogNormal were not fitted but saved as 0 to keep format\n",
    "def OutputCSV(Reco_hist, Energy_test, CSV_name):\n",
    "    results = []\n",
    "    for i_E in range(len(Energy_test)):\n",
    "        mean_gauss,mean_gauss_error,peak_gauss, peak_gauss_error, res_gauss, res_gauss_error = 0, 0, 0, 0, 0, 0\n",
    "        mean_log_normal,mean_log_normal_error,peak_log_normal, peak_log_normal_error, res_log_normal, res_log_normal_error = 0, 0, 0, 0, 0, 0\n",
    "        mean_gamma,mean_gamma_error,peak_gamma, peak_gamma_error, res_gamma, res_gamma_error = fit_all(\"E_reco\",Reco_hist[i_E][\"E_truth\"],Reco_hist[i_E][\"counts\"],Reco_hist[i_E][\"bin_edges\"])\n",
    "        results.append({\n",
    "            \"ParticleEnergy\": Energy_test[i_E],\n",
    "            \"Mean_Gauss\": mean_gauss,\n",
    "            \"Mean_Gauss_Error\": mean_gauss_error,\n",
    "            \"Peak_Gauss\": peak_gauss,\n",
    "            \"Peak_Gauss_Error\": peak_gauss_error,\n",
    "            \"Res_Gauss\": res_gauss,\n",
    "            \"Res_Gauss_Error\": res_gauss_error,\n",
    "            \"Mean_Gamma\": mean_gamma,\n",
    "            \"Mean_Gamma_Error\": mean_gamma_error,\n",
    "            \"Peak_Gamma\": peak_gamma,\n",
    "            \"Peak_Gamma_Error\": peak_gamma_error,\n",
    "            \"Res_Gamma\": res_gamma,\n",
    "            \"Res_Gamma_Error\": res_gamma_error,\n",
    "            \"Mean_LogNormal\": mean_log_normal,\n",
    "            \"Mean_LogNormal_Error\": mean_log_normal_error,\n",
    "            \"Peak_LogNormal\": peak_log_normal,\n",
    "            \"Peak_LogNormal_Error\": peak_log_normal_error,\n",
    "            \"Res_LogNormal\": res_log_normal,\n",
    "            \"Res_LogNormal_Error\": res_log_normal_error\n",
    "        })\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(CSV_name, index=False)\n",
    "#outputfile_name = os.path.join(DataPath_Validate, f\"RecoE_MLP_Fit.csv\")\n",
    "#OutputCSV(Reco_hist_mlp_all[0], Energy_test, outputfile_name)\n",
    "for i_para, para in enumerate(ParameterScan):\n",
    "    if i_para != 0:\n",
    "        continue  # \n",
    "    #outputfile_name = os.path.join(DataPath_Validate, f\"RecoE_MLP_Fit_LR{para:.1f}.csv\")\n",
    "    outputfile_name = os.path.join(DataPath_Validate, f\"RecoE_MLP_LR{para:.0e}_Fit.csv\")\n",
    "    OutputCSV(Reco_hist_mlp_all[i_para], Energy_test, outputfile_name)\n",
    "    print(f\"Saved fit results to {outputfile_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
